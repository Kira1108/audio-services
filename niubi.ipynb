{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7acf76a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe following is the output format of a vad model, the model is a streaming vad model, audio is processed in chunks, for each chunk, we will get a list of list of integers\\nas vad results. to analyze the vad results, we need to merge vad results from different chunks.\\n\\n[[beg1, end1], [beg2, end2], .., [begN, endN]]：The same as the offline VAD output result mentioned above.\\n[[beg, -1]]：Indicates that only a starting point has been detected.\\n[[-1, end]]：Indicates that only an ending point has been detected.\\n[]：Indicates that neither a starting point nor an ending point has been detected.\\nThe output is measured in milliseconds and represents the absolute time from the starting point.\\n\\nWhen processing audio, we need know the speech intervals and silence intervals.\\neach speech interval is a time period starting with user's voice and ending with user's silece.\\n\\nI want to process the results, do you have any recommendations?\\n\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The following is the output format of a vad model, the model is a streaming vad model, audio is processed in chunks, for each chunk, we will get a list of list of integers\n",
    "as vad results. to analyze the vad results, we need to merge vad results from different chunks.\n",
    "\n",
    "[[beg1, end1], [beg2, end2], .., [begN, endN]]：The same as the offline VAD output result mentioned above.\n",
    "[[beg, -1]]：Indicates that only a starting point has been detected.\n",
    "[[-1, end]]：Indicates that only an ending point has been detected.\n",
    "[]：Indicates that neither a starting point nor an ending point has been detected.\n",
    "The output is measured in milliseconds and represents the absolute time from the starting point.\n",
    "\n",
    "When processing audio, we need know the speech intervals and silence intervals.\n",
    "each speech interval is a time period starting with user's voice and ending with user's silece.\n",
    "\n",
    "I want to process the results, do you have any recommendations?\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267971c",
   "metadata": {},
   "source": [
    "# Streaming VAD Result Processor\n",
    "\n",
    "Below is a solution to process streaming VAD results. This implementation handles the various output formats from the VAD model and maintains speech segments across multiple chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a6c090e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingVADProcessor:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 max_silence_ms=1000, \n",
    "                 min_speech_ms=300\n",
    "            ):\n",
    "        self.speech_segments = []  # segments 列表（时间点） [start_ms, end_ms]\n",
    "        self.pending_segment = None  # 当前不完整的segment [start_ms, -1]\n",
    "        self.max_silence_ms = max_silence_ms  # 结束一个段落的最大沉默时间\n",
    "        self.min_speech_ms = min_speech_ms  # 有效语音段的最小长度\n",
    "        self.last_end_time = 0  # 记录上一个ending的时间\n",
    "    \n",
    "    def process_chunk(self, chunk_result, chunk_time_ms):\n",
    "        \"\"\"\n",
    "        Process one chunk of VAD results\n",
    "        \n",
    "        Args:\n",
    "            chunk_result: List of VAD results in the format [[beg1, end1], [beg, -1], [-1, end], ...]\n",
    "            chunk_time_ms: Current chunk time in milliseconds from the start\n",
    "            \n",
    "        Returns:\n",
    "            List of any completed speech segments in this chunk\n",
    "        \"\"\"\n",
    "        completed_segments = []\n",
    "        \n",
    "        # 未检测到任何端点\n",
    "        if not chunk_result:\n",
    "            \n",
    "            # 检查是否有一个挂起的段落已经沉默了太久\n",
    "            vad_status = \"empty\" if not self.pending_segment else \"continue\"\n",
    "            \n",
    "            if self.pending_segment and (chunk_time_ms - self.last_end_time) > self.max_silence_ms:\n",
    "                # 将挂起的segment做ennding\n",
    "                segment = [self.pending_segment[0], self.last_end_time]\n",
    "                \n",
    "                # 如果段落足够长，将其添加到结果中\n",
    "                if segment[1] - segment[0] >= self.min_speech_ms:\n",
    "                    self.speech_segments.append(segment)\n",
    "                    completed_segments.append(segment)\n",
    "                self.pending_segment = None\n",
    "                \n",
    "            return completed_segments\n",
    "        \n",
    "        # 如果检测到端点，处理每个segment\n",
    "        for segment in chunk_result:\n",
    "            \n",
    "            # Case 1: 收到完整的segment\n",
    "            if len(segment) == 2 and segment[0] >= 0 and segment[1] > 0:\n",
    "                \n",
    "                # 将挂起的段落结束\n",
    "                if self.pending_segment:\n",
    "                    # 检查当前segment是一个继续还是一个新的段落\n",
    "                    if abs(segment[0] - self.last_end_time) <= self.max_silence_ms:\n",
    "                        # 如果segment是之前片段的继续，用pending的开始时间，组成一个大的，新的segment\n",
    "                        new_segment = [self.pending_segment[0], segment[1]]\n",
    "                        # 如果新的段落足够长，将其添加到结果中\n",
    "                        if new_segment[1] - new_segment[0] >= self.min_speech_ms:\n",
    "                            self.speech_segments.append(new_segment)\n",
    "                            completed_segments.append(new_segment)\n",
    "                    else: \n",
    "                        \n",
    "                        # 如果当前segment不是之前片段的继续，结束之前的片段\n",
    "                        prev_segment = [self.pending_segment[0], self.last_end_time]\n",
    "                        \n",
    "                        # 检查之前的片段是否足够长\n",
    "                        if prev_segment[1] - prev_segment[0] >= self.min_speech_ms:\n",
    "                            self.speech_segments.append(prev_segment)\n",
    "                            completed_segments.append(prev_segment)\n",
    "                        \n",
    "                        # 加入新的片段\n",
    "                        if segment[1] - segment[0] >= self.min_speech_ms:\n",
    "                            self.speech_segments.append(segment)\n",
    "                            completed_segments.append(segment)\n",
    "                    \n",
    "                    self.pending_segment = None\n",
    "                    \n",
    "                else:\n",
    "                    # 如果没有挂起的段落，直接添加当前段落\n",
    "                    if segment[1] - segment[0] >= self.min_speech_ms:\n",
    "                        self.speech_segments.append(segment)\n",
    "                        completed_segments.append(segment)\n",
    "                \n",
    "                self.last_end_time = segment[1]\n",
    "            \n",
    "            # Case 2: 只检测到一个开始 [beg, -1]\n",
    "            elif len(segment) == 2 and segment[0] >= 0 and segment[1] == -1:\n",
    "                # 如果当前没有pending，则开始一个新的pending\n",
    "                if not self.pending_segment:\n",
    "                    self.pending_segment = [segment[0], -1]\n",
    "                # 如果我们已经有一个pending的段落，我们保留较早的开始时间\n",
    "            \n",
    "            # Case 3: 只检测到一个结束 [-1, end]\n",
    "            elif len(segment) == 2 and segment[0] == -1 and segment[1] > 0:\n",
    "                if self.pending_segment:\n",
    "                    # 此时必有一个挂起的段落\n",
    "                    completed_segment = [self.pending_segment[0], segment[1]]\n",
    "                    # 检查是否要添加到speech段落\n",
    "                    if completed_segment[1] - completed_segment[0] >= self.min_speech_ms:\n",
    "                        self.speech_segments.append(completed_segment)\n",
    "                        completed_segments.append(completed_segment)\n",
    "                    self.pending_segment = None\n",
    "                    self.last_end_time = segment[1]\n",
    "        \n",
    "        return completed_segments\n",
    "    \n",
    "    def get_speech_segments(self):\n",
    "        \"\"\"Return all completed speech segments\"\"\"\n",
    "        return self.speech_segments\n",
    "    \n",
    "    def get_pending_segment(self):\n",
    "        \"\"\"Return the current pending segment if any\"\"\"\n",
    "        return self.pending_segment\n",
    "    \n",
    "    def finalize(self):\n",
    "        \"\"\"Finalize processing and return all segments\"\"\"\n",
    "        # If we have a pending segment, complete it with the last known end time\n",
    "        if self.pending_segment:\n",
    "            segment = [self.pending_segment[0], self.last_end_time]\n",
    "            if segment[1] - segment[0] >= self.min_speech_ms:\n",
    "                self.speech_segments.append(segment)\n",
    "            self.pending_segment = None\n",
    "        \n",
    "        return self.speech_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ee257864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 10)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ac81bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use the StreamingVADProcessor with your current VAD model\n",
    "def process_audio_with_vad(audio_path, chunk_size_ms=200, vad_model=None):\n",
    "    \"\"\"Process an audio file using the StreamingVADProcessor\"\"\"\n",
    "    import soundfile\n",
    "    from funasr import AutoModel\n",
    "    \n",
    "    # Load the VAD model if not provided\n",
    "    if vad_model is None:\n",
    "        vad_model = AutoModel(model=\"fsmn-vad\")\n",
    "    \n",
    "    # Read the audio file\n",
    "    speech, sample_rate = soundfile.read(audio_path)\n",
    "    chunk_stride = int(chunk_size_ms * sample_rate / 1000)\n",
    "    \n",
    "    # Initialize the VAD processor\n",
    "    vad_processor = StreamingVADProcessor(max_silence_ms=400, min_speech_ms=300)\n",
    "    \n",
    "    # Process the audio in chunks\n",
    "    cache = {}\n",
    "    total_chunk_num = int(len(speech-1)/chunk_stride+1)\n",
    "    all_completed_segments = []\n",
    "    \n",
    "    for i in range(total_chunk_num):\n",
    "        # Get the current chunk of audio\n",
    "        speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n",
    "        is_final = i == total_chunk_num - 1\n",
    "        \n",
    "        # Get VAD results for this chunk\n",
    "        import time\n",
    "        res = vad_model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size_ms)\n",
    "        # Process VAD results if there are any\n",
    "        if len(res) > 0 and len(res[0][\"value\"]) > 0:\n",
    "            # Current chunk time in ms from the start\n",
    "            current_time_ms = i * chunk_size_ms\n",
    "            \n",
    "            # Process the chunk results\n",
    "            print(f\"Passing {res[0]['value']}\")\n",
    "            completed_segments = vad_processor.process_chunk(res[0][\"value\"], current_time_ms)\n",
    "            all_completed_segments.extend(completed_segments)\n",
    "            if completed_segments:\n",
    "                print(f\"Completed segment at chunk {i}: {completed_segments}\")\n",
    "    \n",
    "    # Finalize to get any pending segments\n",
    "    final_segments = vad_processor.finalize()\n",
    "    \n",
    "    return final_segments\n",
    "\n",
    "# Example usage:\n",
    "# segments = process_audio_with_vad(\"/path/to/audio.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d975f42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "funasr version: 1.2.6.\n",
      "Downloading Model from https://www.modelscope.cn to directory: /Users/mac/.cache/modelscope/hub/models/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 20:29:23,417 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    }
   ],
   "source": [
    "from funasr import AutoModel\n",
    "\n",
    "model = AutoModel(model=\"fsmn-vad\", disable_update = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "973bccbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rtf_avg: 0.132: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 44.55it/s]                                                                                          \n",
      "rtf_avg: 0.024: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 180.28it/s]                                                                                          \n",
      "rtf_avg: 0.025: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 160.51it/s]                                                                                          \n",
      "rtf_avg: 0.030: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 140.51it/s]                                                                                          \n",
      "rtf_avg: 0.024: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 178.19it/s]                                                                                          \n",
      "rtf_avg: 0.097: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 49.24it/s]                                                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing [[740, -1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rtf_avg: 0.024: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 170.56it/s]                                                                                          \n",
      "rtf_avg: 0.025: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 171.15it/s]                                                                                          \n",
      "rtf_avg: 0.062: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 75.01it/s]                                                                                          \n",
      "rtf_avg: 0.021: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 196.89it/s]                                                                                          \n",
      "rtf_avg: 0.021: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 190.73it/s]                                                                                          \n",
      "rtf_avg: 0.038: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 119.63it/s]                                                                                          \n",
      "rtf_avg: 0.021: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 198.06it/s]                                                                                          \n",
      "rtf_avg: 0.021: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 205.09it/s]                                                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing [[-1, 2060]]\n",
      "Completed segment at chunk 13: [[740, 2060]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rtf_avg: 0.043: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 101.55it/s]                                                                                          \n",
      "rtf_avg: 0.021: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 202.11it/s]                                                                                          \n",
      "rtf_avg: 0.029: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 151.73it/s]                                                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing [[2940, -1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rtf_avg: 0.039: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 101.10it/s]                                                                                          \n",
      "rtf_avg: 0.074: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 64.11it/s]                                                                                          \n",
      "rtf_avg: 0.023: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 180.56it/s]                                                                                          \n",
      "rtf_avg: 0.021: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 196.04it/s]                                                                                          \n",
      "rtf_avg: 0.176: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 27.53it/s]                                                                                          \n",
      "rtf_avg: 0.028: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 114.35it/s]                                                                                          \n",
      "rtf_avg: 0.025: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 171.06it/s]                                                                                          \n",
      "rtf_avg: 0.062: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 75.32it/s]                                                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing [[-1, 4400]]\n",
      "Completed segment at chunk 24: [[2940, 4400]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rtf_avg: 0.022: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 197.63it/s]                                                                                          \n",
      "rtf_avg: 0.030: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 146.22it/s]                                                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing [[4850, -1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rtf_avg: 0.041: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 111.91it/s]                                                                                          \n",
      "rtf_avg: 0.021: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 202.43it/s]                                                                                          \n",
      "rtf_avg: 0.076: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 62.39it/s]                                                                                          \n",
      "rtf_avg: 0.021: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 205.89it/s]                                                                                          \n",
      "rtf_avg: 0.026: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 169.50it/s]                                                                                          \n",
      "rtf_avg: 0.075: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 62.72it/s]                                                                                          \n",
      "rtf_avg: 0.023: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 185.60it/s]                                                                                          \n",
      "rtf_avg: 0.020: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 209.24it/s]                                                                                          \n",
      "rtf_avg: 0.026: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 165.18it/s]                                                                                          \n",
      "rtf_avg: 0.031: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 141.48it/s]                                                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing [[-1, 6640]]\n",
      "Completed segment at chunk 36: [[4850, 6640]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rtf_avg: 0.031: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 124.83it/s]                                                                                          \n",
      "rtf_avg: 0.021: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 200.57it/s]                                                                                          \n",
      "rtf_avg: 0.032: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 138.45it/s]                                                                                          \n",
      "rtf_avg: 0.073: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 62.97it/s]                                                                                          \n",
      "rtf_avg: 0.022: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 192.87it/s]                                                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing [[7780, -1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rtf_avg: 0.044: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 103.68it/s]                                                                                          \n",
      "rtf_avg: 0.310: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 15.77it/s]                                                                                          \n",
      "rtf_avg: 0.022: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 180.56it/s]                                                                                          \n",
      "rtf_avg: 0.020: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 206.15it/s]                                                                                          \n",
      "rtf_avg: 0.033: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 136.17it/s]                                                                                          \n",
      "rtf_avg: 0.024: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 164.10it/s]                                                                                          \n",
      "rtf_avg: 0.686: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 66.21it/s]                                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing [[-1, 9580]]\n",
      "Completed segment at chunk 48: [[7780, 9580]]\n",
      "\n",
      "Final speech segments (start_ms, end_ms):\n",
      "Segment 1: [740, 2060] (duration: 1320ms)\n",
      "Segment 2: [2940, 4400] (duration: 1460ms)\n",
      "Segment 3: [4850, 6640] (duration: 1790ms)\n",
      "Segment 4: [7780, 9580] (duration: 1800ms)\n",
      "\n",
      "Total speech duration: 6370ms (6.37s)\n",
      "Number of speech segments: 4\n",
      "Average segment duration: 1592.50ms (1.59s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with the example file from your VAD model\n",
    "if 'model' in locals() and hasattr(model, 'model_path'):  # Check if model is defined\n",
    "    test_wav_file = f\"/Users/mac/Desktop/audio-services/cache/recording.wav\"\n",
    "    speech_segments = process_audio_with_vad(test_wav_file, vad_model=model)\n",
    "    \n",
    "    print(\"\\nFinal speech segments (start_ms, end_ms):\")\n",
    "    for i, segment in enumerate(speech_segments):\n",
    "        duration_ms = segment[1] - segment[0]\n",
    "        print(f\"Segment {i+1}: {segment} (duration: {duration_ms}ms)\")\n",
    "        \n",
    "    # Calculate statistics\n",
    "    total_duration = sum(seg[1] - seg[0] for seg in speech_segments)\n",
    "    print(f\"\\nTotal speech duration: {total_duration}ms ({total_duration/1000:.2f}s)\")\n",
    "    print(f\"Number of speech segments: {len(speech_segments)}\")\n",
    "    if speech_segments:\n",
    "        avg_duration = total_duration / len(speech_segments)\n",
    "        print(f\"Average segment duration: {avg_duration:.2f}ms ({avg_duration/1000:.2f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2277406a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "funasr version: 1.2.6.\n",
      "Downloading Model from https://www.modelscope.cn to directory: /Users/mac/.cache/modelscope/hub/models/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 20:30:43,295 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    }
   ],
   "source": [
    "from funasr import AutoModel\n",
    "\n",
    "vader = AutoModel(model = 'fsmn-vad', disable_update = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5491cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from funasr_onnx import Fsmn_vad_online\n",
    "\n",
    "vader = Fsmn_vad_online(model_dir = \"/Users/mac/.cache/modelscope/hub/models/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "63d4ec51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[[[-1, 9720]]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[[10880, -1]]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[[-1, 11520]]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[[12520, -1]]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[[-1, 13850]]]\n",
      "[]\n",
      "[[[14440, -1]]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[[-1, 16120]]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[[17380, -1]]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "test_wav_file = f\"/Users/mac/Desktop/audio-services/cache/recording.wav\"\n",
    "cache = {}\n",
    "import numpy as np\n",
    "import soundfile\n",
    "speech, sample_rate = soundfile.read(test_wav_file)\n",
    "chunks = np.array([speech[i:i+3200] for i in range(0, len(speech), 3200)])\n",
    "for i, chunk in enumerate(chunks):\n",
    "    is_final = i == len(chunks) - 1\n",
    "    print(vader(chunk, cache = cache, chunk_size = 200, is_final = is_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "936502f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!open /Users/mac/.cache/modelscope/hub/models/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1a94aad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing model from /Users/mac/.cache/modelscope/hub/models/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch/model.onnx to /Users/mac/.cache/modelscope/hub/models/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch/model_quant.onnx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/mac/.cache/modelscope/hub/models/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader.export(quantize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f4147ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "funasr version: 1.2.6.\n",
      "Downloading Model from https://www.modelscope.cn to directory: /Users/mac/.cache/modelscope/hub/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 15:11:57,519 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n",
      "/opt/anaconda3/lib/python3.12/site-packages/funasr/train_utils/load_pretrained_model.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ori_state = torch.load(path, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "from funasr import AutoModel\n",
    "\n",
    "asr = AutoModel(model = 'paraformer-zh-streaming', disable_update = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c293dd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/onnx/_internal/jit_utils.py:308: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:180.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/onnx/utils.py:663: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:180.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/onnx/utils.py:1186: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:180.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing model from /Users/mac/.cache/modelscope/hub/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online/model.onnx to /Users/mac/.cache/modelscope/hub/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online/model_quant.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing model from /Users/mac/.cache/modelscope/hub/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online/decoder.onnx to /Users/mac/.cache/modelscope/hub/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online/decoder_quant.onnx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/mac/.cache/modelscope/hub/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr.export(quantize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c3fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from funasr_onnx.paraformer_online_bin import Paraformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df2f73df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[{'preds': ('欢迎大', ['欢', '迎', '大'])}]\n",
      "[{'preds': ('家来体', ['家', '来', '体'])}]\n",
      "[{'preds': ('验达摩', ['验', '达', '摩'])}]\n",
      "[{'preds': ('院推出', ['院', '推', '出'])}]\n",
      "[{'preds': ('的语音', ['的', '语', '音'])}]\n",
      "[{'preds': ('识别模', ['识', '别', '模'])}]\n",
      "[{'preds': ('型儿么', ['型', '儿', '么'])}]\n",
      "[{'preds': ('呢', ['呢'])}]\n"
     ]
    }
   ],
   "source": [
    "import soundfile\n",
    "import os\n",
    "chunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms\n",
    "encoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention\n",
    "decoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention\n",
    "model1 = Paraformer(model_dir = \"/Users/mac/.cache/modelscope/hub/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online\",\n",
    "                    quantize = True, \n",
    "                    chunk_size = chunk_size)\n",
    "wav_file = \"/Users/mac/Desktop/audio-services/cache/asr_example.wav\"\n",
    "speech, sample_rate = soundfile.read(wav_file)\n",
    "chunk_stride = chunk_size[1] * 960 # 600ms\n",
    "\n",
    "cache = {}\n",
    "total_chunk_num = int(len((speech)-1)/chunk_stride+1)\n",
    "for i in range(total_chunk_num):\n",
    "    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n",
    "    is_final = i == total_chunk_num - 1\n",
    "    res = model1(audio_in=speech_chunk, param_dict = dict(\n",
    "        is_final=is_final,cache = cache, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)\n",
    "    )\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88ecf9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def load_model():\n",
    "    return Paraformer(\n",
    "        model_dir=\"/Users/mac/Desktop/audio-services/resources/modelscope/hub/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online\", \n",
    "        device = \"cuda\",\n",
    "        quantize = True,\n",
    "        chunk_size = [0,10,5],\n",
    "        disable_update = True)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ParaformerStreaming:\n",
    "    chunk_ms:int = 600 \n",
    "    encoder_chunk_look_back:int = 4\n",
    "    decoder_chunk_look_back:int = 1\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.model = load_model()\n",
    "        self.cache = {}\n",
    "        self.chunk_size = [0, int(self.chunk_ms / 60), int(self.chunk_ms / 120)]\n",
    "        \n",
    "\n",
    "    def new(self):\n",
    "        return ParaformerStreaming(\n",
    "            chunk_ms=self.chunk_ms,\n",
    "            encoder_chunk_look_back=self.encoder_chunk_look_back, \n",
    "            decoder_chunk_look_back=self.decoder_chunk_look_back)\n",
    "\n",
    "    def run(self, speech_chunk, sampling_rate:int =16000, is_final = False):\n",
    "            \n",
    "        speech_chunk = np.array(speech_chunk).astype(\"float32\")\n",
    "        \n",
    "        res = self.model(\n",
    "            audio_in=speech_chunk, \n",
    "            param_dict = dict(\n",
    "                cache = self.cache,\n",
    "                is_final = is_final,\n",
    "                encoder_chunk_look_back = self.encoder_chunk_look_back,\n",
    "                decoder_chunk_look_back = self.decoder_chunk_look_back\n",
    "            ))\n",
    "        if len(res) == 0:\n",
    "            text = \" \"\n",
    "        elif \"preds\" not in res[0]:\n",
    "            text = \" \"\n",
    "        else:\n",
    "            try:\n",
    "                text = res[0]['preds'][0]\n",
    "            except:\n",
    "                text = \" \"\n",
    "        return [{\"text\":text}]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a7fbcf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ParaformerStreaming(chunk_ms=600, encoder_chunk_look_back=4, decoder_chunk_look_back=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7127b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dd0727d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "chunk 0 time: 0.11s\n",
      "[{'text': ' '}]\n",
      " \n",
      "chunk 1 time: 0.04s\n",
      "[{'text': ' '}]\n",
      " \n",
      "chunk 2 time: 0.04s\n",
      "[{'text': '欢迎大'}]\n",
      " \n",
      "chunk 3 time: 0.05s\n",
      "[{'text': '家来体'}]\n",
      " \n",
      "chunk 4 time: 0.05s\n",
      "[{'text': '验达摩'}]\n",
      " \n",
      "chunk 5 time: 0.05s\n",
      "[{'text': '院推出'}]\n",
      " \n",
      "chunk 6 time: 0.05s\n",
      "[{'text': '的语音'}]\n",
      " \n",
      "chunk 7 time: 0.05s\n",
      "[{'text': '识别模'}]\n",
      " \n",
      "chunk 8 time: 0.05s\n",
      "[{'text': '型儿一'}]\n",
      " \n",
      "chunk 9 time: 0.03s\n",
      "[{'text': '是'}]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for i in range(total_chunk_num):\n",
    "    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n",
    "    is_final = i == total_chunk_num - 1\n",
    "    start = time.time()\n",
    "    res = p.run(speech_chunk = speech_chunk, is_final = is_final)\n",
    "    end = time.time()\n",
    "    if len(res) == 0:\n",
    "        text = \" \"\n",
    "    elif \"preds\" not in res[0]:\n",
    "        text = \" \"\n",
    "    else:\n",
    "        try:\n",
    "            text = res[0]['preds'][0]\n",
    "        except:\n",
    "            text = \" \"\n",
    "    print(text)\n",
    "    print(f\"chunk {i} time: {end - start:.2f}s\")\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d9c312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac1f3cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'嗯'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170366ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
